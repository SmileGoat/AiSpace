# config for nezha
includes:
  - "../base.yml"

dataset:
    tokenizer:
        name: bert_tokenizer
        vocab:
            filename: null
            special_tokens:
                PAD: "[PAD]"
                UNK: "[UNK]"
                SEP: "[SEP]"
                CLS: "[CLS]"
                MASK: "[MASK]"
        tokenize_chinese_chars: True
        do_lower_case: True
        do_basic_tokenize: True
        non_split_tokens: null
        max_len: 512

pretrained:
    norm_name: bert
    name: nezha
    adapter: tf_huggingface_bert_adapter
    force_download: false
    init_from_pretrained: true
    cache_dir: /search/data1/yyk/data/pretrained/nezha # your path to save models
    model_path: null
    vocab_path: null
    config_path: null
    config:
        output_attentions: false
        output_hidden_states: false
        layer_norm_eps: 1e-12
        use_position_embedding: false
        use_relative_position: true
    ref: https://github.com/huawei-noah/Pretrained-Language-Model
    family:
        NEZHA-Base:
            model:
                # your/path/to/NEZHA-Base
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Base
                suffix: model.ckpt-900000
                to_insert_paths:
                    - pretrained.model_path
            vocab:
                # your/path/to/NEZHA-Base/vocab.txt
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Base/vocab.txt
                to_insert_paths:
                    - pretrained.vocab_path
                    - dataset.tokenizer.vocab.filename
            config:
                # your/path/to/NEZHA-Base/bert_config.json
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Base/bert_config.json
                to_insert_paths: # set the pretrained.config_path with saved path of this file.
                    - pretrained.config_path
                to_replaces: # replace pretrained.config with the json content.
                    - pretrained.config
        NEZHA-Base-WWM:
            model:
                # your/path/to/NEZHA-Base-WWM
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Base-WWM
                suffix: model.ckpt-691689
                to_insert_paths:
                    - pretrained.model_path
            vocab:
                # your/path/to/NEZHA-Base-WWM/vocab.txt
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Base-WWM/vocab.txt
                to_insert_paths:
                    - pretrained.vocab_path
                    - dataset.tokenizer.vocab.filename
            config:
                # your/path/to/NEZHA-Base-WWM/bert_config.json
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Base-WWM/bert_config.json
                to_insert_paths: # set the pretrained.config_path with saved path of this file.
                    - pretrained.config_path
                to_replaces: # replace pretrained.config with the json content.
                    - pretrained.config
        NEZHA-Large:
            model:
                # your/path/to/NEZHA-Large
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Large
                suffix: model.ckpt-325810
                to_insert_paths:
                    - pretrained.model_path
            vocab:
                # your/path/to/NEZHA-Large/vocab.txt
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Large/vocab.txt
                to_insert_paths:
                    - pretrained.vocab_path
                    - dataset.tokenizer.vocab.filename
            config:
                # your/path/to/NEZHA-Large/bert_config.json
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Large/bert_config.json
                to_insert_paths: # set the pretrained.config_path with saved path of this file.
                    - pretrained.config_path
                to_replaces: # replace pretrained.config with the json content.
                    - pretrained.config
        NEZHA-Large-WWM:
            model:
                # your/path/to/chinese_wwm
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Large-WWM
                suffix: model.ckpt-346400
                to_insert_paths:
                    - pretrained.model_path
            vocab:
                # your/path/to/chinese_wwm/vocab.txt
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Large-WWM/vocab.txt
                to_insert_paths:
                    - pretrained.vocab_path
                    - dataset.tokenizer.vocab.filename
            config:
                # your/path/to/chinese_wwm/bert_config.json
                url: /search/data1/yyk/data/pretrained/nezha/NEZHA-Large-WWM/bert_config.json
                to_insert_paths: # set the pretrained.config_path with saved path of this file.
                    - pretrained.config_path
                to_replaces: # replace pretrained.config with the json content.
                    - pretrained.config

