includes:
  - "../pretrain/bert_wwm.yml"

model_name: bert_for_qa

model_attributes:
    hidden_dropout_prob: 0.5
    initializer_range: 0.02
    hidden_size: 1024
    pos_emb_size: 768
    attention_probs_dropout_prob: 0.5
    num_attention_heads: 4

training:
  learning_rate: 3e-5
  max_epochs: 10
  batch_size: 16
  callbacks:
    # callback name
    early_stopping:
      switch: true
      config:
        patience: 2

dataset:
  name: "lstc_2020/DuEE_role"
  data_dir: "./data"
  transformer: "lstc_2020/DuEE_role_as_qa"

  source:
    train: "train"
    validation: "validation[:50%]"
    test: "validation[-50%:]"

  tokenizer:
    max_len: 150

  inputs:
    - name: input_ids
      column: input_ids
      type: LIST_OF_INT
      max_len: 150
    - name: token_type_ids
      column: token_type_ids
      type: LIST_OF_INT
      max_len: 150
    - name: attention_mask
      column: attention_mask
      type: LIST_OF_INT
      max_len: 150

  outputs:
    - name: output_1
      column: start_positions
      type: LIST_OF_INT
      num: 150
      weight: 1.0
      loss:
        name: sigmoid_cross_entropy
#        config:
#          from_logits: true
#          reduction: "sum"
      metrics:
        - name: categorical_accuracy
    - name: output_2
      column: end_positions
      type: LIST_OF_INT
      num: 150
      weight: 1.0
      loss:
        name: sigmoid_cross_entropy
#        config:
#          from_logits: true
#          reduction: "sum"
      metrics:
        - name: categorical_accuracy
    - name: output_3
      column: is_impossible
      type: LIST_OF_INT
      num: 2
      weight: 0.0001
      loss:
        name: sigmoid_cross_entropy
#        config:
#          from_logits: true
#          reduction: "sum"
      metrics:
        - name: categorical_accuracy

pretrained:
    name: chinese_roberta_wwm_large_ext
    init_from_pretrained: true


